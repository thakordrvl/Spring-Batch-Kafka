---

```markdown
# ğŸ“¦ Spring Batch CSV to Kafka Integration

A Spring Boot application that integrates **Spring Batch** to read data from a **CSV file** and publish it to a **Kafka** topic. The batch job can be triggered through a REST API endpoint, making it ideal for automated and scalable data pipelines.

---

## ğŸš€ Features

- âœ… **CSV File Reading**: Reads records using Spring Batchâ€™s `FlatFileItemReader`.
- ğŸ” **Kafka Integration**: Publishes processed data to a Kafka topic via `KafkaTemplate`.
- ğŸŒ **REST API Trigger**: Exposes a simple API endpoint to start the batch job.
- ğŸ§© **Custom Serialization**: Configures Kafka producers to serialize `Person` objects correctly.
- ğŸ› ï¸ **Modular Design**: Clear separation of concerns for easier testing and extension.

---

## ğŸ› ï¸ Technologies Used

| Component      | Description                     |
|----------------|---------------------------------|
| Spring Boot    | Application framework           |
| Spring Batch   | Batch processing engine         |
| Apache Kafka   | Distributed event streaming     |
| REST Controller| For triggering batch jobs       |
| Maven          | Build and dependency management |

---

## ğŸ“‚ Project Structure

```

src
â”œâ”€â”€ main
â”‚   â”œâ”€â”€ java
â”‚   â”‚   â””â”€â”€ com.example.batchkafka
â”‚   â”‚       â”œâ”€â”€ config
â”‚   â”‚       â”‚   â””â”€â”€ KafkaConfig.java
â”‚   â”‚       â”œâ”€â”€ controller
â”‚   â”‚       â”‚   â””â”€â”€ JobLauncherController.java
â”‚   â”‚       â”œâ”€â”€ job
â”‚   â”‚       â”‚   â””â”€â”€ CsvToKafkaJobConfig.java
â”‚   â”‚       â”œâ”€â”€ model
â”‚   â”‚       â”‚   â””â”€â”€ Person.java
â”‚   â”‚       â””â”€â”€ BatchKafkaApplication.java
â”‚   â””â”€â”€ resources
â”‚       â”œâ”€â”€ application.properties
â”‚       â””â”€â”€ data.csv

````

---

## âš™ï¸ Setup and Installation

### 1. Clone the Repository

```bash
git clone <repository-url>
cd <repository-folder>
````

### 2. Configure Kafka

Update `src/main/resources/application.properties` with your Kafka details:

```properties
spring.kafka.bootstrap-servers=localhost:9092
spring.kafka.topic.name=person-topic
```

Make sure Kafka and Zookeeper are running locally or reachable remotely.

### 3. Build the Project

```bash
mvn clean install
```

### 4. Run the Application

```bash
mvn spring-boot:run
```

---

## ğŸ” REST API Endpoint

### â–¶ï¸ Trigger Batch Job

* **Method**: `GET`
* **Endpoint**: `/joblauncher/job1`
* **Description**: Starts the Spring Batch job which reads from a CSV file and sends records to Kafka.

**Example Usage:**

```bash
curl http://localhost:8080/joblauncher/job1
```

---

## ğŸ’¬ Kafka Producer Configuration Example

```java
@Configuration
public class KafkaConfig {

    @Bean
    public ProducerFactory<String, Person> producerFactory() {
        Map<String, Object> configProps = new HashMap<>();
        configProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        configProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
        configProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, JsonSerializer.class);
        return new DefaultKafkaProducerFactory<>(configProps);
    }

    @Bean
    public KafkaTemplate<String, Person> kafkaTemplate() {
        return new KafkaTemplate<>(producerFactory());
    }
}
```

---

## ğŸ“„ CSV Format

Ensure your `data.csv` file (placed in `src/main/resources`) follows this structure:

```csv
firstName,lastName,email
John,Doe,john.doe@example.com
Jane,Smith,jane.smith@example.com
```

---

## ğŸ“Œ Notes

* The batch job reads from `data.csv` each time it's triggered.
* You can extend the `Person` model or add processors/validators as needed.

---

## âœ… Future Enhancements

* [ ] Add Job Status & History tracking
* [ ] Support for multiple file formats
* [ ] Kafka consumer for downstream validation

---
---

```

---
```
